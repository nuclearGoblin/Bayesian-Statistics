---
title: "Homework4"
author: "Kitty Harris"
output:
  pdf_document: default
---

### Problem 1:
**Voter turnout has been a popular talking point in recent elections. To get a sense of voter turnout in Colorado, a national polling company contacted randomly selected registered Colorado voters until they reached 200 persons who said that they had voted. 303 persons said they had not voted, for a total sample size of 503. The response $y$ is the number of failures before getting the 200th registered voter who had voted. Thus, $y$ can be modeled as a negative binomial distribution with $n$ successes and probability of success (a registered voter stating they voted), i.e., $y|n$~Neg-Bin$(n,\theta)$, and**  

$$
p(y|n,\theta) = \frac{\Gamma(y+n)}{\Gamma(n)y!}\theta^n(1-\theta)^y
$$

**Based on what you've heard, you believe that the true proportion of registered voters that actually vote is less than 50%, but you're not super confident about this. Thus, you chose a Beta(1.1, 1.5) prior distribution for $\theta$. Let $q(\theta|y) = p(y|\theta)p(\theta)$.**

**Implement a rejection sampler for the posterior distribution.**

#### a. Determine a elevant proposal distribution. What proposal distribution will you use? You are NOT allowed to use a uniform distribution. Why did you choose this one? \break

First, we find our unnormalized posterior explicitly:

$$q(\theta|y) = \frac{\Gamma(y+n)}{\Gamma(n)y!}\theta^n(1-\theta)^y \frac{\theta^{0.1}(1-\theta)^{0.5}}{B(1.1,1.5)}$$

The first fraction is less than 1 for our known values of $y$ and $n$, as is $\theta^n(1-\theta)^y$ for $\theta \in (0,1)$ -- therefore, the unnormalized posterior will be enclosed by a Beta(1.1,1.5) distribution. The Beta(1.1,1.5) distribution should also have the same support as the unnormalized posterior, and because it has a form matching part of the posterior, it will be easy to determine $M$.  

\newpage

#### b. Determine a scale constant $M$ to create an appropriate bounding function $Mg(\theta)$. Create a plot of $Mg(\theta)$ and $q(\theta|y)$ versus $\theta$, making sure to clearly distinguish the two functions. \break

The ratio is:

$$\frac{q(\theta|y)}{g(\theta)} = \frac{\Gamma(y+n)}{\Gamma(n)\Gamma(y+1)}\theta^n(1-\theta)^y$$

so we maximize $\theta^{200}(1-\theta)^{303}$ to find the value of $\theta$ at which we will evaluate $M$:

```{r}
to_maximize <- function(x){x^200*(1-x)^303}
thmax = optimize(to_maximize,interval=c(0,1),maximum=TRUE)$maximum
M = exp(lgamma(503)-lgamma(200)-lgamma(304))*to_maximize(thmax)
print(M)

Mg <- function(x){M*dbeta(x,shape1=1.1,shape2=1.5)}
q <- function(x){to_maximize(x)*exp(lgamma(503)-lgamma(200)-lgamma(304))*dbeta(x,shape1=1.1,shape2=1.5)}

theta = seq(0, 1, length.out=1000)
plot(theta,Mg(theta),ylab="probability density",type='l',col='black')
lines(theta,q(theta),col='blue')
legend("topright",legend=c("proposal","unnormalized posterior"),col=c("black","blue"),lty=1)
```

\newpage

#### c. Run the rejection sampler so that 100,000 samples are accepted. Then create a plot showing the true posterior density (the normalizing constant was determined in the previous homework) versus the approximate density coming from the rejection sampler, making sure to clearly distinguish the two functions. \break

We generate our samples, then run the rejection algorithm on each generated sample.

```{r}
nsamp = 100000 #Start by generating the number of samples we need
list = rbeta(nsamp-length(list),shape1=1.1,shape2=1.5)
for(i in 1:length(list)){ #Now run initial rejection portion of algorithm
  reject = runif(1,0,Mg(list[i]))
  if(reject>q(list[i])){ #Generate a random value uniform between 0 and the unnormalized posterior.
    temp = TRUE
    while(temp){
      x = rbeta(1,shape1=1.1,shape2=1.5) #Prepare to replace entry
      if(runif(1,0,Mg(x))<=q(x)){        #And see if we need to reject again
        temp = FALSE
        list[i] = x
      }
    }
  }
}

theta = seq(0,1,length.out=10000)
hist(as.numeric(list),ylab='probability density',xlab='theta',freq=FALSE,
     main='',lty='blank',col='blue',
     breaks=seq(min(list),max(list),length.out=50),ylim=c(0,20),xlim=c(0.3,0.5))
yconst <- integrate(q,lower=0,upper=1)$value
lines(theta,q(theta)/yconst,lwd=2)

legend("topright",legend=c("calculated","a"),lwd=c(2,NA),pch=c(NA,15),
       col=c('black','blue'))
```

### Problem 2:
**Suppose that the unnormalized bivariate posterior distribution for parameters $\theta_1$ and $\theta_2$ is $q(\theta_1,\theta_2|y) = \theta_1^2exp(-\theta_1\theta_2^2-\theta_2^2+2\theta_2-4\theta_1)\;\theta_1>0,\theta_2\in(-\infty,\infty)$.**

#### a. Derive the full conditional distribution for $\theta_1$.

The unnormalized full conditional distribution is:
$$
p(\theta_1|\theta_2,y) \propto \theta_1^2e^{-(\theta_2^2+4)\theta_1}I(\theta_1>0).
$$
This is the kernel of a Gamma distribution; $\theta_1|\theta_2,y$~Gamma$(3,\theta_2^2+4)$

\newpage

#### b. Derive the full conditional distribution for $\theta_2$.

We start by finding the kernel:

$$\begin{aligned}
p(\theta_2|\theta_1,y) & \propto e^{-(\theta_1+1)\theta_2^2+2\theta_2} \\
& = exp\left[-\frac{1}{2}\left(2(\theta_1+1)\theta_2^2-4\theta_2\right)\right] \\
& = exp\left[-\frac{1}{2}2(\theta_1+1)\left(\theta_2^2-\frac{2}{\theta_1+1}\theta_2\right)\right] \\
& \propto exp\left[-\frac{1}{2}2(\theta_1+1)\left(\theta_2^2-\frac{2}{\theta_1+1}\theta_2+\frac{1}{(\theta_1+1)^2}\right)\right]
\end{aligned}$$

So $\theta_2|\theta_1,y$~N$\left((\theta_1+1)^{-1},(2(\theta_1+1))^{-1/2}\right)$.

\newpage

#### c. Run a Gibbs sampler for $\theta_1$ and $\theta_2$ for 100,000 cycles. Plot the posterior distributions of $\theta_1$ and $\theta_2$ (individually).

We don't have data, so I will choose the initial values somewhat arbitrarily.

```{r}
B = 100000 #n cycles
theta1 = 15; theta2 = 1; #use theta2 = 1 and theta1 = mean(Gamma(3,5))
theta1post = numeric(B); theta2post = numeric(B)
#run a Gibbs sampler for B cycles
for (i in 1:B){
  theta1 = rgamma(1,3,theta2^2+1); theta1post[i] = theta1
  theta2 = rnorm(1,1/(theta1+1),sqrt(1/(2*(theta1+1)))); theta2post[i] = theta2
}

hist(theta1post,ylab='probability density',xlab='theta1',freq=FALSE,main='',
     breaks=seq(min(theta1post),max(theta1post),                                                                  length.out=50))
```
\newpage
```{r}
hist(theta2post,ylab='probability density',xlab='theta2',freq=FALSE,main='',
     breaks=seq(min(theta2post),max(theta2post),length.out=50))
```

\newpage

### Problem 3: 
**Consider the data in coal.txt, which is counts for the number of coal mine disasters over a 112-year period (1851 to 1962) in the United Kingdom. The data have relatively high disaster counts in the early era, and relatively low counts in the later era. When did technology improvements and safety practices have an actual effect on the rate of serious accidents? We assume that our data can be
described as having a Poisson($\lambda$) distribution for the first $k$ years, and a Poisson($\phi$) distribution for the years $k+1,...,112$. In this model, $k$ is the year that the "change" occurred when technology and/or safety improvements changed the pattern of coal mine disasters. We assume that within years $1,...,k$ the number of disasters is i.i.d. Poisson($\lambda$), and the number of disasters in years $k+1,...,112$ is i.i.d. Poisson($\phi$). Our interest lies in finding the posterior distributions for $\lambda$,$\phi$, and $k$. Assume a Gamma(4, 1) prior distribution for $\lambda$, a Gamma(1, 2) prior distribution for $\phi$ and a a discrete uniform prior distribution on [1,2, . ,112] for $k$. A discrete uniform prior distribution means that each possible value in the sample space has equal probability of occurring.**

First, the data distribution is:

$$\begin{aligned}
& p(y_i|k,\lambda,\phi) & =&  
  \begin{cases}
    \frac{e^{-\lambda}\lambda^{y_i}}{y_i!} & i\leq k\\
    \frac{e^{-\phi}\phi^{y_i}}{y_i!} & i>k
  \end{cases} \\
\therefore & p(y|k,\lambda\phi) &=& \prod_{i=1}^k\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\prod_{i=k+1}^{112}\frac{e^{-\phi}\phi^{y_i}}{y_i!} \\
& & =& e^{-k\lambda}e^{-(112-k)\phi}\lambda^{\sum_{i=1}^ky_i}\phi^{\sum_{i=k+1}^{112}y_i}\prod_{i=1}^{112}\frac{1}{y_i!}
\end{aligned}$$

making the posterior:

$$\begin{aligned}
p(k,\lambda,\phi|y) & \propto p(y|k,\lambda,\phi)p(k,\lambda,\phi) = p(y|k,\lambda,\phi)p(k)p(\lambda)p(\phi)
\end{aligned}$$

where

$$
p(k) = \frac{1}{112} \\
p(\lambda) = \frac{1^{-4}\lambda^{4-1}e^{-\lambda/1}}{\Gamma(4)} = \frac{\lambda^3e^{-\lambda}}{6} \\
p(\phi) = \frac{2^{-1}\phi^{1-1}e^{-\phi/2}}{\Gamma(1)} = \frac{e^{-\phi/2}}{2}
$$
#### a. Determine the full conditional distribution for $\lambda$. It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $k$. \break

We start by dropping the $p(k),p(\phi)$ terms as they will not have terms depending on $\lambda$, then drop other non-$\lambda$ terms. 

$$\begin{aligned}
p(\lambda|k,\phi,y) &\propto e^{-k\lambda}e^{-(112-k)\phi}\lambda^{\sum_{i=1}^ky_i}\phi^{\sum_{i=k+1}^{112}y_i}\frac{\lambda^3e^{-\lambda}}{6}\prod_{i=1}^{112}\frac{1}{y_i!} \\
&\propto e^{-(k+1)\lambda}\lambda^{3+\sum_{i=1}^ky_i}, \\
\end{aligned}$$

so $\lambda|k,\phi,y$ ~ Gamma$\left(4+\sum_{i=1}^ky_i,\frac{1}{k+1}\right)$.

#### b. Determine the full conditional distribution for $\phi$. It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $k$. \break

We start by dropping the $p(k),p(\lambda)$ terms as they will not have terms depending on $\phi$, then drop other non-$\phi$ terms. 

$$\begin{aligned}
p(\phi|k,\lambda,y) &\propto e^{-k\lambda}e^{-(112-k)\phi}\lambda^{\sum_{i=1}^ky_i}\phi^{\sum_{i=k+1}^{112}y_i}\frac{e^{-\phi/2}}{2}\prod_{i=1}^{112}\frac{1}{y_i!} \\
&\propto e^{-(112-k)\phi}\phi^{\sum_{i=k+1}^{112}y_i}e^{-\phi/2} \\
&= e^{-(112-k+\frac{1}{2})\phi}\phi^{\sum_{i=k+1}^{112}y_i},
\end{aligned}$$

so $\phi|k,\lambda,y$ ~ Gamma$\left(1+\sum_{i=k+1}^{112}y_i,\frac{1}{112-k+1/2}\right)$

#### c. The full conditional distribution for $k$ is
$$
p(k|\lambda,\phi,y) = \frac{\exp(k(\phi-\lambda))\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^ky_i}}{\sum_{k=1}^{112}\left(\exp(k(\phi-\lambda))\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^ky_i}\right)}
$$
**Run a 100,000 cycle Gibbs sampler using the distributions determined in parts 1 through 3 to approximate the posterior distribution of the three parameters.**

```{r}
B = 100000 #n cycles
dat = read.delim("coal.txt",header=TRUE,sep=" "); y = dat["disasters"]

lambda = 1; phi = 1; k = 1;
probs = function(k,lam,phi){
  list = numeric(nrow(y))
  for(i in 1:nrow(y)){
    list[i] = exp(k*(phi-lam))*(lam/phi)^(sum(head(y,k)))
  }
  #list = list/sum(list) #normalize #do this later actually
  return(list)
}

kgen = runif(B,0,1) #pre-generate our k generators
lambdapost = numeric(B); phipost = numeric(B); kpost = numeric(B)
for (i in 1:B){ #run a Gibbs sampler for B cycles
  lambda = rgamma(1,4+sum(head(y,k)),1/(k+1)); lambdapost[i] = lambda
  phi = rgamma(1,1+sum(head(y,k)),1/(112-k+1/2)); phipost[i] = phi
  #generate the greatest k point with probability <= the generated prob
  kprob = 0; knew = 0 #start k=0 because we'll get an erroneous +1
  problist = probs(k,lambda,phi); probstotal = sum(problist)
  while(kprob<kgen[i]){ 
    kprob = (kprob*probstotal + problist[i])/probstotal
    knew = knew+1
  }
  k = knew; kpost[i] = k
}

hist(lambdapost,ylab='probability density',xlab='lambda',freq=FALSE,main='',
     breaks=seq(min(lambdapost),max(lambdapost),                                                                  length.out=50))
hist(phipost,ylab='probability density',xlab='phi',freq=FALSE,main='',
     breaks=seq(min(phipost),max(phipost),                                                                  length.out=50))
hist(kpost,ylab='probability density',xlab='k',freq=FALSE,main='',
     breaks=seq(min(kpost),max(kpost),                                                                  length.out=50))

```