---
title: "Homework3"
author: "Kitty Harris"
output:
  pdf_document: default
  html_notebook: default
---

### Problems 1-5:

**Voter turnout has been a popular talking point in recent elections. To get a sense of voter turnout in Colorado, a national polling company contacted randomly selected registered Colorado voters until they reached 200 persons who said that they had voted. 303 persons said they had not voted, for a total sample size of 503. The response $y$ is the number of failures before getting the 200th registered voter who had voted. Thus, $y$ can be modeled as a negative binomial distribution with $n$ successes and probability of success (a registered voter stating they voted), i.e., $y|n$~ Neg-Binom$(n,\theta)$, and**

$$
p(y|n,\theta) = \frac{\Gamma(y+n)}{\Gamma(n)y!}\theta^n(1-\theta)^y.
$$

**Based on what you've heard, you believe that the true proportion of registered voters that actually vote is less than 50%, but you're not super confident about this. Thus, you chose a Beta(1.1, 1.5) prior distribution for $\theta$.**

\newpage <!-- keep plot with question -->

### Problem 1:

**Create a graphic of the plotting the likelihood function and the prior density versus $\theta$. Recall that the likelihood function is the data density evaluated at the observed data values as a function of $\theta$. Make sure to scale the likelihood function so that its mode is similar to the mode of the prior. Make sure to provide a legend distinguishing the two functions.**

```{R}
theta = seq(0, 1, length.out=100)
likelihood = function(th){ #unnormalized
  return(th^200*(1-th)^303*.9e147)
}
prior = function(th){
  return(dbeta(th,shape1=1.1,shape2=1.5))
}
#print(gamma(503)/(gamma(200)*gamma(304)))
#print(likelihood(theta))
plot(theta,likelihood(theta),ylab='Unnormalized Probability')
points(theta,prior(theta),col='blue')
legend(.8,1.3,c("likelihood","prior"),c("black","blue"))
```
\newpage

### Problem 2:
**Determine the MAP estimate of $\theta$.**

We will find the maximum by taking the first and second derivatives.

$$\begin{aligned}
& \frac{dp}{d\theta} \propto n\theta^{n-1}(1-\theta)^y-y\theta^n(1-\theta)^{y-1}
\therefore & n\theta^{n-1}(1-\theta)^y = y\theta^n(1-\theta)^{y-1} \\
\therefore & n(1-\theta) = y\theta \\
\therefore & n - n\theta = y\theta \\
\therefore & n = (y+n)\theta \\
\therefore & \theta = \frac{n}{y+n}
\end{aligned}$$

So there is a point of interest at $\theta = \frac{n}{y+n}$; now check if it is a maximum:

$$
\frac{d^2p}{d\theta^2} = \frac{\Gamma(y+n)}{\Gamma(n)y!}\left(n(n-1)\theta^{n-2}(1-\theta)^y+ny\theta^{n-1}(1-\theta)^{y-1}-ny\theta^{n-1}(1-\theta)^{y-1}+y(y-1)\theta^n(1-\theta)^{y-2}\right)
$$
Which, evaluated at $\hat{\theta}$, is:
$$\begin{aligned}
& \frac{\Gamma(y+n)}{\Gamma(n)y!}\left(n(n-1)\left(\frac{n}{y+n}\right)^{n-2}\left(1-\frac{n}{y+n}\right)^{y}+y(y-1)\left(\frac{n}{y+n}\right)^n\left(1-\frac{n}{y+n}\right)^{y-2}\right) \\
= & \frac{\Gamma(y+n)}{\Gamma(n)y!}\left(1-\frac{n}{y+n}\right)^{y-2}\left(\frac{n}{y+n}\right)^{n-2}\left(n(n-1)\left(1-\frac{n}{y+n}\right)^2+y(y-1)\left(\frac{n}{y+n}\right)^{2}\right)
\end{aligned}
$$

Since $y$ is odd and $\frac{n}{y+n} > 1$, this is negative, indicating that we have found a maximum, so $\hat{\theta}_{MAP} = \frac{n}{y+n} = 503/200$.

\newpage <!-- keep plot with question -->

### Problem 3:
**Determine the posterior mean and variance using deterministic methods. Note that you will need to determine the scaling constant associated with the marginal data density, $p(y)$.**

#### a. What is $p(y)$? Specifically, determine this constant numerically.

We wish to normalize $p(\theta|y) \propto p(y|\theta)p(\theta)$.

```{r}
dpost <- function(theta,const=1,y=303,n=200){
  coeff = exp(lgamma(y+n)-lgamma(y)-lgamma(y+1))*const #use lgammas to prevent underflow
  return(coeff*theta^n*(1-theta)^y*dbeta(theta,shape1=1.1,shape2=1.5))
}
yconst <- integrate(dpost,lower=0,upper=1)$value #Since the normalizing factor is 1/p(y), don't need to invert
print(yconst)
```

#### b. What is the posterior mean?

We can integrate to find the posterior mean.

```{r}
mean_target <- function(theta,const){return(theta*dpost(theta,const))}
mean <- integrate(mean_target,lower=0,upper=1,const=1/yconst)
print(mean)
```

#### c. What is the posterior variance?

We can find the variance using $Var(x) = E(X^2) - E(X)^2$.

```{r}
second_target <- function(theta,const){return(theta^2*dpost(theta,const))}
var <- integrate(second_target,lower=0,upper=1,const=1/yconst)$value - mean$value^2
print(var)
```

\newpage <!-- keep plot with question -->

### Problem 4:
**Determine the mean and variance of the normal approximation of the posterior. Use the MAP estimate to compute the mean and observed information.**

We have already found the mean of the normal approximation as it is the MAP estimate, $\frac{n}{y+n} = \frac{503}{200}$. The variance of the normal approximation is $\frac{\hat{\theta}_{MAP}^2}{y} = \frac{503^2}{200^2303}$. 

```{r}
thatmap = 200/503
sigsqhat = thatmap^2/303
print(sigsqhat)
```
The observed information of the approximation is:

$$\begin{aligned}
-\frac{\partial^2}{\partial \theta^2}\ln\left(\sqrt\frac{y}{2\pi\theta^2}e^{-y(y-\theta)^2/2\theta^2}\right)\biggr\rvert_{\hat{\theta}}
& = -\frac{\partial^2}{\partial \theta^2}\left(c-\ln\theta-\frac{y(y^2-2y\theta+\theta^2)}{2\theta^2}\right)\biggr\rvert_{\hat{\theta}} \\
& = \frac{\partial^2}{\partial \theta^2}\left(ln\theta + \frac{y^3}{2\theta^2} -\frac{y}{\theta}+\frac{1}{2}\right)\biggr\rvert_{\hat{\theta}} \\
& = \frac{\partial}{\partial \theta}\left(\frac{1}{\theta} - \frac{y^3}{\theta^3} + \frac{y}{2\theta^2} \right)\biggr\rvert_{\hat{\theta}} \\
& = \left(-\frac{1}{\theta^2} +3 \frac{y^3}{\theta^4} - \frac{y}{\theta^3}\right)\biggr\rvert_{\hat{\theta}} \\ 
& = \frac{n^2}{(y+n)^2} + 3\frac{n^4y}{(y+n)^4} - \frac{n^3y}{(n+y)^3} 
= \frac{503^2}{200^2} + 3\frac{503^4303}{200^4} - \frac{503^3303}{200^3}
\end{aligned}$$

```{r}
503^2/200^2+3*503^4*303/200^4-503^3*303/200^3
```

And the observed information of the "true" posterior is:

$$\begin{aligned}
-\frac{\partial^2}{\partial \theta^2}\ln\left(\frac{\Gamma(y+n)}{\Gamma(n)y!}\theta^n(1-\theta)^y\right)\biggr\rvert_{\hat{\theta}} 
& = -\frac{\partial^2}{\partial \theta^2}\left(c+n\ln\theta+y\ln(1-\theta)\right)\biggr\rvert_{\hat{\theta}} \\
& = -\frac{\partial}{\partial \theta}\left(\frac{n}{\theta}-\frac{y}{1-\theta}\right)\biggr\rvert_{\hat{\theta}} \\
& = -\left(-\frac{n}{\theta^2}-\frac{y}{(1-\theta)^2}\right)\biggr\rvert_{\hat{\theta}} \\
& = \frac{n(y+n)^2}{n^2} + \frac{y}{(1-\frac{n}{y+n})^2} = \frac{503^2}{200} + \frac{303}{(1-\frac{200}{503})^2}
\end{aligned}$$

```{r}
503^2/200 +303/(1-(200/503))^2
```
\newpage

### Problem 5:
**Plot the true posterior distribution and normal approximation in a single plot. Make sure to distinguish the two densities. How well do they match?**

```{r}
napproxpost <- function(theta,thetahat,sigsqhat){
  unnormalized <- function(theta){dnorm(theta,mean=thetahat,sd=sqrt(sigsqhat))}
  cnorm <- integrate(unnormalized,lower=0,upper=1)$value
  return(unnormalized(theta)/cnorm) #in this case, y IS a sum of several 1s and 0s.
}
print(thatmap)

plot(theta,dpost(theta,const=1/yconst),ylab="density",type="l")
lines(theta,napproxpost(theta,thatmap,sigsqhat),col='blue')
legend("topright",legend=c("true","approximate"),col=c("black","blue"),lty=1)
```

These are a very close match. The approximate is very slightly wider, making it a little bit shorter, but the means line up and the variances are similar.

\newpage

### Problem 6:
**More fun with the Jeffreys’ prior. Let $y$ ~ Poisson$(\theta)$. We have previously shown that Jeffreys' prior for this setting is $p(\theta) \propto \theta^{-1/2}I(\theta>0)$.**

#### a. Let $\phi = \sqrt\theta$. Determine the Fisher’s Information for $\phi$. What is Jeffreys' prior for $\phi$?
Start by substituting $\phi$ into the Poisson distribution, then get the Jeffreys' prior directly:

$$\begin{aligned}
& p(y|\phi) & = & \frac{e^{-\phi^2}\phi^{2y}}{y!} \\
\therefore & ln p(y|\phi) & = & -\phi^2 + 2y\ln\phi - \ln(y!) \\
\therefore & \frac{d}{d\phi}\ln p(y|\theta) & = & -2\phi+\frac{2y}{\phi} \\
\therefore & \frac{d^2}{d\phi^2}\ln p(y|\theta) & = & -2 - \frac{2y}{\phi} \\
\therefore & E\left(\frac{d^2}{d\phi^2}\ln p(y|\phi)|\phi\right) & = & -2 - \frac{2}{\phi}E(y|\phi) \\
&& = & -2-\frac{2}{\phi}e^{-\phi^2}\sum_{y=0}^\infty y\frac{\phi^{2y}}{y!} = -2-\frac{2}{\phi^2}e^{-\phi^2}\phi^2e^{\phi^2} \\
\therefore & J(\phi) = -E\left(\frac{d^2}{d\phi^2}\ln p(y|\phi)|\phi\right) & = & 2+2 = 4
\end{aligned}$$
with $\phi \in (0,\infty)$, making Jeffreys' prior $p(\phi) = 2I(\theta>0)$.

#### b. Use the change of variable formula on $p_\phi(\phi)$ to find $p_\theta(\theta)$.

$$\begin{aligned}
& \frac{d\phi}{d\theta} = \frac{1}{2\sqrt\theta} \\
\therefore & \frac{d\phi}{d\theta}\sqrt{J(\phi)} = \frac{1}{2\sqrt\theta}2I(\theta>0) = \theta^{-1/2}I(\theta>0),
\end{aligned}$$

as expected.

#### c. What does this example confirm about Jeffreys’ prior?

This example confirms that Jeffreys' prior follows our rule $\sqrt{J(\phi)} = \frac{d\theta}{d\phi}\sqrt{J(\theta)}$, allowing us to transform our variable of interest while still getting consistent results.